{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4eefaf-4496-47a2-b717-3e0f8d4984e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7031d3b3-40da-4c9b-a182-aed439290c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataset):\n",
    "    vocab = set([])\n",
    "    for document in dataset:\n",
    "        vocab = vocab | set(document)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f239322-62e7-4396-a4e4-a81b002a373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsToVec(vocab, inputset):\n",
    "    vector = [0]*len(vocab)\n",
    "    for word in inputset:\n",
    "        if word in vocab:\n",
    "            vector[vocab.index(word)] = 1\n",
    "        else:\n",
    "            print(\"Word not found in vocabulary: \", word)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feead74c-4eb4-4204-ad5c-25f8601f7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(training_mat, labels):\n",
    "    training_mat = array(training_mat)\n",
    "    num_docs = len(training_mat)\n",
    "    num_words = len(training_mat[0])\n",
    "    p_spam = sum(labels) / float(num_docs)\n",
    "    p0_num = ones(num_words)\n",
    "    p1_num = ones(num_words)\n",
    "    p0_denom = 2.0\n",
    "    p1_denom = 2.0\n",
    "\n",
    "    for i in range(num_docs):\n",
    "        if labels[i] == 1:\n",
    "            p1_num += training_mat[i]\n",
    "            p1_denom += sum(training_mat[i])\n",
    "        else:\n",
    "            p0_num += training_mat[i]\n",
    "            p0_denom += sum(training_mat[i])\n",
    "\n",
    "    p1vect = log(p1_num / p1_denom)\n",
    "    p0vect = log(p0_num / p0_denom)\n",
    "\n",
    "    return p0vect, p1vect, p_spam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70d6f5e-613b-4d43-b7c2-2355d488d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(vec_to_classify, p0vec, p1vec, pclass1):\n",
    "    p1 = sum(vec_to_classify*p1vec) + log(pclass1)\n",
    "    p0 = sum(vec_to_classify*p0vec) + log(1.0 - pclass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6386c3-5632-43e2-87c3-718b49cfa7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(given_string):\n",
    "    tokens_list = re.split(r'\\W+', given_string)\n",
    "    return [t.lower() for t in tokens_list if len(t) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8d05c0-bae2-4889-b3f3-c03f99731bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    full_text = []\n",
    "    #I have 25 non-spam (ham) emails and 25 spam emails -> hardcoding from 1 to 25. In other case, parse the folder and then read files\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open(f\"email/spam/{i}.txt\").read())\n",
    "        docList.append(wordList)\n",
    "        full_text.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open(f\"email/ham/{i}.txt\").read())\n",
    "        docList.append(wordList)\n",
    "        full_text.extend(wordList)\n",
    "        classList.append(0)\n",
    "\n",
    "    vocab_list = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        #lets add 10 files from training to test set\n",
    "        random_ind = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[random_ind])\n",
    "        del(trainingSet[random_ind])\n",
    "\n",
    "    training_matrix = []\n",
    "    training_class = []\n",
    "    for i in trainingSet:\n",
    "        training_matrix.append(wordsToVec(vocab_list, docList[i]))\n",
    "        training_class.append(classList[i])\n",
    "    p0V, p1V, pSpam = trainNaiveBayes(array(training_matrix), array(training_class))\n",
    "    error = 0\n",
    "    for i in testSet:\n",
    "        word_vector = wordsToVec(vocab_list, docList[i])\n",
    "        pred = classify(array(word_vector), p0V, p1V, pSpam) \n",
    "        if pred != classList[i]:\n",
    "            print(f\"Doc {i} misclassified: predicted {pred}, actual {classList[i]}\")\n",
    "            error += 1\n",
    "    print(f\"The error rate is {float(error)/len(testSet)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd73c3a-c533-440f-b7ea-0c033f3f956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 32 misclassified: predicted 0, actual 1\n",
      "The error rate is 0.1\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e70e85-b8db-4612-9a4d-e34a27a6efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 48 misclassified: predicted 0, actual 1\n",
      "The error rate is 0.1\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2154f064-f936-44b6-9bfd-b64adc216086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is 0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249c156-5b95-4769-9bae-4b098940d995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
