{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4eefaf-4496-47a2-b717-3e0f8d4984e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7f65b-a63e-4245-9294-af90ccb44c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64739-6bca-4157-b0f2-1db82fc5ddcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b70d6f5e-613b-4d43-b7c2-2355d488d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(vec_to_classify, p0vec, p1vec, pclass1):\n",
    "    p1 = sum(vec_to_classify*p1vec) + log(pclass1)\n",
    "    p0 = sum(vec_to_classify*p0vec) + log(1.0 - pclass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feead74c-4eb4-4204-ad5c-25f8601f7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(training_mat, labels):\n",
    "    num_docs = len(training_mat)\n",
    "    num_words = len(training_mat[0])\n",
    "    p_abusive = sum(labels)/float(num_docs)\n",
    "    p0_num = ones(num_words)\n",
    "    p1_num = ones(num_words)\n",
    "    p0denom = 2.0\n",
    "    p1denom = 2.0\n",
    "    for i in range(num_docs):\n",
    "        if labels[i] == 1:\n",
    "            p1_num += training_mat[i]\n",
    "            p1denom += sum(training_mat[i])\n",
    "        else:\n",
    "            p0_num += training_mat[i]\n",
    "            p0denom += sum(training_mat[i])\n",
    "    p1vect = log(p1_num/p1denom)\n",
    "    p0vect = log(p0_num/p0denom)\n",
    "    return p0vect, p1vect, p_abusive\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7971e01-2ff7-4505-911a-0515a81af6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems','help', 'please'],\n",
    "                  ['maybe','not','take','him','to','park','stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    labels = [0,1,0,1,0,1]\n",
    "    return postingList, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7031d3b3-40da-4c9b-a182-aed439290c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataset):\n",
    "    vocab = set([])\n",
    "    for document in dataset:\n",
    "        vocab = vocab | set(document)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f239322-62e7-4396-a4e4-a81b002a373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsToVec(vocab, inputset):\n",
    "    vector = [0]*len(vocab)\n",
    "    for word in inputset:\n",
    "        if word in vocab:\n",
    "            vector[vocab.index(word)] = 1\n",
    "        else:\n",
    "            print(\"Word not fount in vocabulary: \", word)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5030df-f467-4fac-8a1c-49845a6d5276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4becf8-d3e1-4404-8a71-36fc69205779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3e015-d9cf-44f1-9187-794b58fcfbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5ce28-63d5-4a24-9aae-9d6f83f0cbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce144b95-dc85-484f-ab04-313c7789bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702875f-b39a-46a1-9734-e9690142a52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29798bab-419d-4e37-a2cb-e4e878ed8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts, labels = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddba9fb0-3d08-461d-befc-0b3ca6fa068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab = createVocabList(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7241e720-1eea-4143-9365-69ee6a996b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['help',\n",
       " 'my',\n",
       " 'mr',\n",
       " 'food',\n",
       " 'buying',\n",
       " 'stupid',\n",
       " 'posting',\n",
       " 'licks',\n",
       " 'problems',\n",
       " 'park',\n",
       " 'has',\n",
       " 'maybe',\n",
       " 'garbage',\n",
       " 'how',\n",
       " 'worthless',\n",
       " 'cute',\n",
       " 'ate',\n",
       " 'steak',\n",
       " 'take',\n",
       " 'dog',\n",
       " 'so',\n",
       " 'I',\n",
       " 'to',\n",
       " 'love',\n",
       " 'please',\n",
       " 'is',\n",
       " 'quit',\n",
       " 'stop',\n",
       " 'dalmation',\n",
       " 'flea',\n",
       " 'not',\n",
       " 'him']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdcfe64-6bde-46c8-b32f-85c36ea79354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsToVec(my_vocab,posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d70f9e-c223-4b9a-9587-0b4731b74275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = []\n",
    "for i in posts:\n",
    "    train_mat.append(wordsToVec(my_vocab,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e46aa9f-35dd-4777-8058-86f311949585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9d632bc-87c3-450f-96d2-73a8b65ea1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNaiveBayes(train_mat,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4599cee1-b5c0-4702-a93b-0056784211a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07692308, 0.15384615, 0.07692308, 0.03846154, 0.03846154,\n",
       "       0.03846154, 0.03846154, 0.07692308, 0.07692308, 0.03846154,\n",
       "       0.07692308, 0.03846154, 0.03846154, 0.07692308, 0.03846154,\n",
       "       0.07692308, 0.07692308, 0.07692308, 0.03846154, 0.07692308,\n",
       "       0.07692308, 0.07692308, 0.07692308, 0.07692308, 0.07692308,\n",
       "       0.07692308, 0.03846154, 0.07692308, 0.07692308, 0.07692308,\n",
       "       0.03846154, 0.11538462])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e2cbb71-8d52-41a4-9917-9367debfa449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eaf7446-dbb5-4e17-8a2a-2e657d7e2f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93cd20e4-3ded-4ec6-9a7f-8af736c2b27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.05, 0.05, 0.1 , 0.1 , 0.2 , 0.1 , 0.05, 0.05, 0.1 , 0.05,\n",
       "       0.1 , 0.1 , 0.05, 0.15, 0.05, 0.05, 0.05, 0.1 , 0.1 , 0.05, 0.05,\n",
       "       0.1 , 0.05, 0.05, 0.05, 0.1 , 0.1 , 0.05, 0.05, 0.1 , 0.1 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6386c3-5632-43e2-87c3-718b49cfa7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
